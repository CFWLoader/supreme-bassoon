# 全国高校绿色计算大赛决赛阶段

笔者负责第三阶段的题，此题是数据挖掘的题，输入是一个二分类的文件，输出是测试集为某一类的概率。

由于比赛仅可使用`Numpy`和`Pandas`两个库，失去了`sklearn`的支援，只能在决赛手搓一些比较容易实现的预测概率算法了。

## 最开始的候选方案

由于是从一个二分类问题转换为一个预测样本为某一个类的概率，笔者首先想出`回归树`、`（高斯）朴素贝叶斯`以及`Logistic回归`模型三个模型。

随后也想到了`k近邻`算法，从实现难度来看，笔者较为熟悉的朴素贝叶斯和Logistic回归较为容易实现，k近邻作为备选。

## Logistic回归的实现与性能评估

因为想到Logistic回归可以产生`概率`，实现难度也不高，所以最先实现了该模型。函数也很简单，就是在线性回归的输出套一层`sigmoid`函数，在做最优化时候的导数也很简单，顺带实现了一个简单的最速下降法。

因为有一些特征的值域范围比较大，所以对每个特征进行了`Min-Max/正态归一化`，问题是最开始时`Numpy`的使用错误，使得归一化错误，Logistic在提交上去回馈的AUC在`0.3`之间。

后来改正了归一化的代码，果然好了，AUC达到了`0.8`。

## 回归树与朴素贝叶斯的尝试

笔者太久没有写过树系列了，不能保证在完赛前写出来，加上队里有大佬在写着，于是很快地从网上拷贝现成的代码，以验证自己的想法，然而网上抠下来的练习代码实在太差，20秒都没出结果，笔者就没提交上去看效果了。次方案作废。

由于很多特征属于连续性变量，笔者便简单地实现了高斯朴素贝叶斯模型来做预测，然而模型出来地概率仅仅只能在内部比较，最后还是输出概率最大的类，概率值不能作为最后的预测值。方案作废。

## 尝试优化Logistic回归

笔者觉得有些特征不但不贡献正面的影响，反而拖累模型的预测，于是闪过用`PCA`来选取有效的特征，然而事与愿违，选取出来的特征并没有提高最后的AUC得分，方案作废。

## 完赛后的优化方案

完赛之后，笔者突然想到在做朴素贝叶斯的时候，看到正类与负类的比例极不平衡，大约有1：3的比例，也许可以对Logistic回归优化中的时候，加大正类的权重。然而比赛已经结束了，再怎么玩都只能测训练集的AUC了。